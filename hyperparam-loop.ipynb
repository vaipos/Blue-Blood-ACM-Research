{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "import os\n",
    "from io import BytesIO\n",
    "\n",
    "boto_session = boto3.Session(region_name='us-east-1')\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "\n",
    "role = \"arn:aws:iam::211125439249:role/service-role/AmazonSageMaker-ExecutionRole-20250314T153928\"\n",
    "role_name = role.split('/')[-1]  # Extract just the role name from the ARN\n",
    "\n",
    "# Attach AdministratorAccess policy to your existing role\n",
    "iam_client = boto3.client('iam')\n",
    "iam_client.attach_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyArn=\"arn:aws:iam::aws:policy/AdministratorAccess\"\n",
    ")\n",
    "print(f\"Attached AdministratorAccess policy to role: {role}\")\n",
    "\n",
    "# Assume blood.csv is in S3 already - if not, upload it first\n",
    "input_data_s3_uri = \"s3://blue-blood-data/final_df.csv\"\n",
    "\n",
    "bucket_name = \"blue-blood-data\"\n",
    "region = 'us-east-1'\n",
    "file_key = 'final_df.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# First, make sure train.py exists in the notebook directory\n",
    "if not os.path.exists(\"train.py\"):\n",
    "    print(\"Error: train.py not found in the current directory\")\n",
    "else:\n",
    "    print(\"Found train.py in the current directory\")\n",
    "    \n",
    "    # Upload train.py to S3 to ensure latest version is used\n",
    "    code_prefix = \"code\"\n",
    "    s3_code_path = sagemaker_session.upload_data(\"train.py\", bucket=bucket_name, key_prefix=code_prefix)\n",
    "    print(f\"Uploaded train.py to {s3_code_path}\")\n",
    "\n",
    "# Verify that the S3 data file exists\n",
    "try:\n",
    "    s3_client = boto3.client('s3', region_name=region)\n",
    "    s3_client.head_object(Bucket=bucket_name, Key=file_key)\n",
    "    print(f\"Verified that s3://{bucket_name}/{file_key} exists\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Error verifying S3 file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Define hyperparameter grid for comprehensive testing\n",
    "param_grid = {\n",
    "    'epochs': [10, 20, 50],\n",
    "    'learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'lstm_units': [32, 64, 128],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "# Store results for comparison\n",
    "results = []\n",
    "\n",
    "# Loop through combinations (or a subset if you want to limit testing)\n",
    "# For demonstration, using only some combinations to reduce run time\n",
    "combinations_to_test = [\n",
    "    {'epochs': 10, 'learning_rate': 0.001, 'lstm_units': 64, 'dropout_rate': 0.2},\n",
    "    {'epochs': 20, 'learning_rate': 0.001, 'lstm_units': 128, 'dropout_rate': 0.2},\n",
    "    {'epochs': 10, 'learning_rate': 0.01, 'lstm_units': 64, 'dropout_rate': 0.1}\n",
    "    # Add more combinations as needed\n",
    "]\n",
    "\n",
    "print(f\"Starting hyperparameter testing with {len(combinations_to_test)} combinations\")\n",
    "\n",
    "for idx, params in enumerate(combinations_to_test):\n",
    "    # Extract parameters\n",
    "    epochs = params['epochs']\n",
    "    learning_rate = params['learning_rate']\n",
    "    lstm_units = params['lstm_units']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    \n",
    "    # Create a unique job name based on parameters\n",
    "    job_name = f\"lstm-e{epochs}-lr{learning_rate}-u{lstm_units}-d{int(dropout_rate*10)}-b1-{idx}\"\n",
    "    job_name = job_name.replace('.', 'd')  # Replace dots with 'd' for valid job name\n",
    "    print(f\"\\nStarting job {idx+1}/{len(combinations_to_test)}: {job_name}\")\n",
    "    \n",
    "    # Create the TensorFlow estimator with this set of hyperparameters\n",
    "    estimator = TensorFlow(\n",
    "        entry_point='train.py',\n",
    "        role=role,\n",
    "        instance_count=1,\n",
    "        instance_type='ml.m5.xlarge',\n",
    "        framework_version='2.9',\n",
    "        py_version='py39',\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        hyperparameters={\n",
    "            'epochs': epochs,\n",
    "            'learning_rate': learning_rate,\n",
    "            'lstm_units': lstm_units,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'job_name': job_name\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # Start training job\n",
    "    estimator.fit({'train': input_data_s3_uri})\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'job_name': job_name,\n",
    "        'parameters': {**params},\n",
    "        'model_s3_path': f\"s3://{bucket_name}/models/{job_name}/lstm_model.pkl\",\n",
    "        'chart_s3_path': f\"s3://{bucket_name}/models/{job_name}/training-validation-loss.png\"\n",
    "    })\n",
    "    \n",
    "    # Optional: Download and display the chart for this model\n",
    "    try:\n",
    "        chart_key = f\"models/{job_name}/training-validation-loss.png\"\n",
    "        response = s3.get_object(Bucket=bucket_name, Key=chart_key)\n",
    "        image_data = response['Body'].read()\n",
    "        image = BytesIO(image_data)\n",
    "        \n",
    "        img = plt.imread(image)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Model: {job_name}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not display chart for {job_name}: {e}\")\n",
    "    \n",
    "    break\n",
    "\n",
    "# Print summary of all tested models\n",
    "print(\"\\nHyperparameter Testing Summary:\")\n",
    "for result in results:\n",
    "    print(f\"Job: {result['job_name']}\")\n",
    "    print(f\"Parameters: {result['parameters']}\")\n",
    "    print(f\"Model path: {result['model_s3_path']}\")\n",
    "    print(f\"Chart path: {result['chart_s3_path']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define the bucket name and the file (graph) you want to fetch\n",
    "s3_graph_key = 'models/lstm-e10-lr0d001-u64-d2-b1-0/training-validation-loss.png'\n",
    "\n",
    "# Fetch the file from S3 into memory\n",
    "response = s3.get_object(Bucket=bucket_name, Key=s3_key)\n",
    "image_data = response['Body'].read()\n",
    "image = BytesIO(image_data)\n",
    "\n",
    "# Display the image using matplotlib\n",
    "img = plt.imread(image)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "s3_pkl_key = 'models/lstm-e10-lr0d001-u64-d2-b1-0/lstm_model.pkl'\n",
    "\n",
    "# Get the object from S3\n",
    "response = s3.get_object(Bucket=bucket_name, Key=s3_pkl_key)\n",
    "        \n",
    "# Create a binary stream and load with pickle\n",
    "with BytesIO(response['Body'].read()) as f:\n",
    "    obj = pickle.load(f)\n",
    "\n",
    "print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bb_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
